	1.	Chen, J., Han, X., Ma, Y., Zhou, X., & Xiang, L. (2024). Unlock the Correlation between Supervised Fine-Tuning and Reinforcement Learning in Training Code Large Language Models. arXiv preprint arXiv:2406.10305.  ￼
	2.	NVIDIA Developer. (2024). Applying Mixture of Experts in LLM Architectures. Retrieved from https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/  ￼
	3.	Kumar, V. (2025). Training code generation models to debug their own outputs. Amazon Science. Retrieved from https://www.amazon.science/blog/training-code-generation-models-to-debug-their-own-outputs  ￼
	4.	DataCamp. (2024). What Is OpenAI’s Reinforcement Fine-Tuning? Retrieved from https://www.datacamp.com/blog/reinforcement-fine-tuning  ￼
	5.	IBM. (2024). What is mixture of experts? Retrieved from https://www.ibm.com/think/topics/mixture-of-experts  ￼
	6.	Hugging Face. (2024). Fine tune with SFTTrainer - Intermediate. Retrieved from https://discuss.huggingface.co/t/fine-tune-with-sfttrainer/67311  ￼
	7.	OpenAI Developer Forum. (2023). How does fine tuning really work? Retrieved from https://community.openai.com/t/how-does-fine-tuning-really-work/39972  ￼
	8.	Hugging Face. (2024). smol-course/1_instruction_tuning/supervised_fine_tuning.md at main. Retrieved from https://github.com/huggingface/smol-course/blob/main/1_instruction_tuning/supervised_fine_tuning.md  ￼
	9.	Fergougui, H. (2024). Mixture of Experts for Faster, More Efficient LLMs. Medium. Retrieved from https://medium.com/@hamzafergougui/mixture-of-experts-for-faster-more-efficient-llms-426351d942e6  ￼
	10.	XueFuzhao. (2024). awesome-mixture-of-experts. GitHub. Retrieved from https://github.com/XueFuzhao/awesome-mixture-of-experts  ￼
